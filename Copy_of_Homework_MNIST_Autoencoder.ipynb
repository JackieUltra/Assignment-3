{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackieUltra/Assignment-3/blob/main/Copy_of_Homework_MNIST_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAWVbhB16p-U"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Homework: Autoencoding MNIST and Celebrity Faces\n",
        "\n",
        "\n",
        "> **Due Date: March 5th, 2025 @ 1:00pm**\n",
        ">\n",
        "> Please turn in this completed notebook via Google classroom. Email clay.smyth@ucsf.edu if you run into any issues.\n",
        "\n",
        "**Collaboration policy and more**\n",
        "\n",
        "You're welcome (and highly encouraged) to work with and discuss this homework assignment with others in the class, and feel free to use any resources (textbooks, online notebooks, etc). The only requirement is that the final notebook that you turn in must be your own written work (no copy and pasting, please).\n",
        "\n",
        "**Overview**\n",
        "\n",
        "In class, we cover how Hinton and Salakhutdinov's 2006 Science Paper, [\"Reducing the Dimensionality of Data with Neural Networks\"](https://www.science.org/doi/10.1126/science.1127647) was one of the first demonstrations of unsupervised pretraining for use in training deep neural networks. In this homework, we'll implement autoencoders in the context of MNIST. Additionally, as an optional assignment, a similar architecture can be used for a subset of CelebA dataset of celebrity faces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px8y6lmq6p-V"
      },
      "source": [
        "## Before you get started\n",
        "\n",
        "**1) Background reading**\n",
        "\n",
        "Please Read Hinton and Salakhutdinov's 2006 seminal work on deep autoencoders (https://www.science.org/doi/10.1126/science.1127647), as this notebook aims to recreate this important work. A few questions to think about as you read that will help you in this assignment:\n",
        "  - What architecture do they use for their deep autoencoders?\n",
        "  - Why were deep neural networks so much harder to train in 2006?\n",
        "\n",
        "  Limitations in computational power.\n",
        "\n",
        "**2) How to run this notebook**\n",
        "\n",
        "This Jupyter Notebook can be used in two ways:\n",
        "* *Option 1: Download the notebook*\n",
        "\n",
        "  We've included all the imports necessary for this homework. Please make sure you're running Python 3 with PyTorch (and Torchvision) installed and ready to go, along with NumPy and Matplotlib. Although you might find that these models train a bit faster on GPU, this homework assignment should be doable on most modern laptops. If you're having trouble please let us know ASAP.\n",
        "\n",
        "* *Option 2: Run it online on Google Colaboratory*\n",
        "\n",
        "  - Colab gives access to a GPU, so it could be useful in case you don't have CUDA installed on your computer (**Note: you can use this as an opportunity to get started on GPU training, but we recommend you develop your model and make sure everything works on CPU first**)\n",
        "  - Make a copy of this notebook in your Google Drive folder: \"File\" -> \"Save a copy in Drive...\"\n",
        "  - By default, Colab does not make GPUs available, but you can easily access them by selecting GPU in \"Runtime\" -> \"Change runtime type...\"\n",
        "  - Remember that Colab runs in a temporary virtual machine, so all the data created while running the notebook will be lost at the end of the session, or when the runtime disconnects due to inactivity. To preserve data between sessions, there are a couple of options:\n",
        "    * you can link Colab to your personal Google Drive by mounting it on your runtime, see first cell below.\n",
        "    * you can download/upload files from the Files tab on the right sidebar.\n",
        "\n",
        "**3) How to complete this assignment**\n",
        "\n",
        "  - Fill out the relevant code blocks as indicated\n",
        "  - Answer questions by writing them directly in the text block. Please keep your written answers concise, most are meant to be answered in a sentence or two.\n",
        "  - Make figures showing your results and add comments with your observations.\n",
        "\n",
        "**4) Optional exercise: CelebA Data**\n",
        "\n",
        "Whereas MNIST is a toy dataset built into PyTorch, we can also examine a more complex feature space using a subset of 90,000 celebrity portraits from CelebA (see [Liu et al. (2014), \"Deep Learning Face Attributes in the Wild\"](https://arxiv.org/abs/1411.7766)). This is an optional part of the homework, but is a nice way to see how autoencoders perform on other types of visual data. There will be a .zip file of the relevant celebrity faces dataset on the Google Classroom link.\n",
        "\n",
        "***Let's start!***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykN_eZdJ6p-f"
      },
      "source": [
        "## Train an autoencoder on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzyVPwhkMWk3"
      },
      "source": [
        "The following command can be used to mount your personal Google Drive folder on the temporary virtual machine, so you can recover data between sessions (follow the instructions, you'll need an authorization code). Additional info [here](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "STvHSt3zICjF"
      },
      "outputs": [],
      "source": [
        "# # Skip this cell if running locally\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fgkNXdEr6p-X"
      },
      "outputs": [],
      "source": [
        "# Import all the necessary libraries\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es5SJPnnVISe"
      },
      "source": [
        "You shouldn't need CUDA for this assignment, but if you want a head start, or if you just want to see the difference between using a CPU versus a GPU, set `use_cuda = True` below.\n",
        "You can check if CUDA is available on your computer with: `torch.cuda.is_available()`\n",
        "\n",
        "If you are working on Colab, make sure to activate the GPU (\"Runtime\" -> \"Change runtime type...\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKT2zNJR6Rwj",
        "outputId": "70af99d6-79eb-471f-fba9-af1c329176f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "28wNsWmC6Q63"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBrzn_tnFKsM"
      },
      "source": [
        "> **Question 0.1) Why is it important to set the seed for the random number generator?**\n",
        "\n",
        "For both reproducability and to make sure whatever I randomize is not changing every time I run random.anything\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUNoP7qY6p-g"
      },
      "source": [
        "### 1. MNIST Dataset\n",
        "\n",
        "As noted in class, MNIST has been widely used to benchmark new deep learning architectures and is already built into PyTorch. We provide this data as a starting point, again noting that the mean and std of the training set are calculated to be 0.1307 and 0.3081, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H20Wcmu6p-h",
        "outputId": "dfc41e8c-6b9c-4eed-c210-1e5266bc07e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./bmi219_downloads/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.45MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./bmi219_downloads/MNIST/raw/train-images-idx3-ubyte.gz to ./bmi219_downloads/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./bmi219_downloads/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 128kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./bmi219_downloads/MNIST/raw/train-labels-idx1-ubyte.gz to ./bmi219_downloads/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./bmi219_downloads/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 241kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./bmi219_downloads/MNIST/raw/t10k-images-idx3-ubyte.gz to ./bmi219_downloads/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./bmi219_downloads/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.41MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./bmi219_downloads/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./bmi219_downloads/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    './bmi219_downloads', train=True, download=True,\n",
        "    transform=preprocessing)\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    './bmi219_downloads', train=False, download=True,\n",
        "    transform=preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oay1ii680sfO",
        "outputId": "bcdc680b-a2ca-4620-c5fb-f60057d90f23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_dataset[123][0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnPPEsT96p-l"
      },
      "source": [
        "> **Q1.1) How many examples do the training set and test set have?**\n",
        "\n",
        "> 60k train set 10k test set\n",
        "...\n",
        "\n",
        "> **Q1.2) What's the format of each input example? Can we directly put these into a fully-connected layer?**\n",
        "\n",
        "> 1 x 28 x 28, We could put this directly into a fully connected layer but this would be computationally costly and unlikely to hleo the preformance of our model much.\n",
        "\n",
        "\n",
        "> **Q1.3) Why do we normalize the input data for neural networks?**\n",
        "\n",
        "> Normalizing that data allows us to ensure that the model does not pick up on features that are more refective of how the data is demonstrated to the model instead of the data itself\n",
        "...\n",
        "\n",
        "> **Q1.4) In this scenario, MNIST is already split into a training set and a test set. What is the purpose of dataset splitting (and specifically, the purpose of a test set)? For modern deep learning, a three-way split into training, validation, and test sets is usually preferred, why?**\n",
        "\n",
        "> Train your data on a training set allows you to tune and finetune your model which can then be validated on a test set to see perfrmance and start to see some degree of generalizability (given the set). Once this step occurs you should not change your model or you will begain to overfit to your test set ie data leakage will occur. By spliting the data into three sets you are able to get a few test set iterations on independent data before you validate a final model on the test set.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr83wABJ6p-m"
      },
      "source": [
        "### 2. Using DataLoaders for MNIST\n",
        "\n",
        "Set up the DataLoader objects below. Although the arguments are prepopulated, you may need to change the batch sizes or other arguments during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Df46Ok2t6p-n"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64  # <-- Please change this as necessary\n",
        "NUM_WORKERS = 4  # <-- Use more workers for more CPU threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Tx21NwCGqwE-"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvgrdGuhdwKt",
        "outputId": "1f327c94-50e0-49aa-d4bd-5618d0e455cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "one_batch=next(iter(train_loader))\n",
        "one_batch[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "k-jvDlIhFCkz",
        "outputId": "3d449a86-913d-4e23-9c7b-6b9eba66381b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get one batch of data\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)  # Get the first batch\n",
        "\n",
        "# Select one image in batch\n",
        "image = images[0]\n",
        "label = labels[0]\n",
        "\n",
        "# Convert image tensor to numpy for visualization\n",
        "if isinstance(image, torch.Tensor):\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "# Plot image\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwHSkI99G9it",
        "outputId": "a487720f-0571-4149-b83c-13b33c2ac9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of images: <class 'torch.Tensor'>\n",
            "Shape of images: torch.Size([64, 1, 28, 28])\n",
            "Type of labels: <class 'torch.Tensor'>\n",
            "Shape of labels: torch.Size([64])\n",
            "Shape of one image: torch.Size([1, 28, 28])\n",
            "Label: 9\n"
          ]
        }
      ],
      "source": [
        "# Get one batch of data\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)  # Get the first batch\n",
        "\n",
        "# Print information\n",
        "print(f\"Type of images: {type(images)}\")  # Should be a Tensor\n",
        "print(f\"Shape of images: {images.shape}\")  # (batch_size, C, H, W)\n",
        "\n",
        "print(f\"Type of labels: {type(labels)}\")  # Should be a Tensor or list\n",
        "print(f\"Shape of labels: {labels.shape if isinstance(labels, torch.Tensor) else 'Not a Tensor'}\")\n",
        "\n",
        "# Check a single image and label\n",
        "image = images[0]\n",
        "label = labels[0]\n",
        "\n",
        "print(f\"Shape of one image: {image.shape}\")  # (C, H, W)\n",
        "print(f\"Label: {label}\")  # Should be an integer or tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jL6Zs5z6p-r"
      },
      "source": [
        "> **Q2.1) It's recommended to shuffle the training data over each epoch, but this isn't typically the case for the test set, why?**\n",
        "\n",
        "> We want to make sure the model does not learn the order of the dataset when training. This does not matter when testing. In addition by shuffling the test set we may have slighly different prefromances in on our test set each time we run whihc can be confusing.\n",
        "\n",
        "...\n",
        "\n",
        "> **Q2.2) What seems to be a good batch size for training? What happens if you train with a batch size of 1? What about a batch size equal to the total training set?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q2.3) The PyTorch DataLoader object is an iterator that generates batches as it's called. Try to pull a few images from the training set to see what these images look like. Does the DataLoader return only the images? What about the labels?**\n",
        "\n",
        "> DataLoader returns both the image and the label.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMlogSbL6p-s"
      },
      "source": [
        "### 3. Define your neural network architecture\n",
        "\n",
        "With your data and dataloaders appropriately set, you're ready to define a network architecture. In this homework, we'll ask you to evaluate two different architectures.\n",
        "\n",
        "For the first (we'll call it `HNet` in this homework), please implement Hinton's 2006 architecture of 7-hidden layers:\n",
        "\n",
        "```[784 x 1000 x 500 x 250 x 2 x 250 x 500 x 1000 x 784]. ```\n",
        "\n",
        "For the second, implement your own autoencoder architecture, `MyNet`, again using a bottleneck dimension of 2. As a note, the larger your model, the longer it will take to train. Can you achieve similar performance to the model above using a more condensed model?\n",
        "\n",
        "**Tips:**\n",
        "* Try different activation functions (Tanh, Sigmoid, ReLU, etc)\n",
        "* A sequence of layers can be defined more easily using `nn.Sequential`, see [docs](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)\n",
        "* Split your network into an `.encoder()` and a `.decoder()`, that will be called sequentially in `.forward()`. This will be useful later on when we'll ask to visualize the low-dimensional embeddings (\"latent space\") produced by the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "gGK2j8UM6p-t"
      },
      "outputs": [],
      "source": [
        "class HNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 250),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(250,2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2, 250),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(250, 500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 784),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten the input\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        x = torch.unflatten(x, dim=1, sizes=(1, 28, 28))  # Unflatten the output\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "O_Grvigg6p-x"
      },
      "outputs": [],
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 500),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(500, 250),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(250,2),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2, 250),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(250, 500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten the input\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        x = torch.unflatten(x, dim=1, sizes=(1, 28, 28))  # Unflatten the output\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQ-CAnlYad2"
      },
      "source": [
        "> **Q3.1) What activation functions did you use, and why?**\n",
        "\n",
        "> I wanted to expirement with the leaky ReLU and also have no activation on my bottleneck to avoid limiting latent space. Mostly it was just to try something different.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2rGw2FD6p-2"
      },
      "source": [
        "### 4. Write your own training function\n",
        "\n",
        "Write your own training function that takes your **model**, an **optimizer**, and a **training criterion**, and iterates over the **training set**.\n",
        "* *Hint*: Because an autoencoder is a form of unsupervised learning, we won't need to use the labels like in the MNIST classification example. Keep in mind the format of the images and whether they're compatible with feed-forward networks.\n",
        "* For each epoch, print and record (in an array or list) the training loss.\n",
        "* You may want to save the model and its weights on file at regular intervals ([checkpointing](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference)). In order to visualize the autoencoder's learning process, we suggest to save at least three timepoints: early, intermediate, and final (for instance, if your model converges after 60 epochs, save your model at 5 epochs, 30 epochs, and 60 epochs).\n",
        "\n",
        "A few useful tips:\n",
        "- Feel free to look at the MNIST classification notebook from previous recitations and use it as a template.\n",
        "- Printing out the intermediate variables and their shape at each step can be helpful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "o2vn3O8L6p-2"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, optimizer, criterion, n_epochs, checkpoint_epochs=[5, 30, 60], save_dir=\"./checkpoints\"):\n",
        "    \"\"\"\n",
        "    Trains an autoencoder model.\n",
        "\n",
        "    Parameters:\n",
        "    - train_loader: DataLoader for the training dataset.\n",
        "    - model: The autoencoder model.\n",
        "    - optimizer: Optimization algorithm (e.g., Adam, SGD).\n",
        "    - criterion: Loss function (e.g., MSELoss).\n",
        "    - n_epochs: Number of training epochs\n",
        "    - checkpoint_epochs: List of epochs where model checkpoints will be saved.\n",
        "    - save_dir: Directory to save model checkpoints.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Store training loss over time\n",
        "    loss_history = []\n",
        "\n",
        "    # Create directory for saving models\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Starting training on {device} for {n_epochs} epochs...\\n\")\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, _ in train_loader:  # Ignore labels\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)  # Compare reconstruction with original\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Compute average loss for this epoch\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        loss_history.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch}/{n_epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # Save model checkpoint at specified epochs\n",
        "        if epoch in checkpoint_epochs:\n",
        "            checkpoint_path = os.path.join(save_dir, f\"autoencoder_epoch_{epoch}.pth\")\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return loss_history  # Return loss history for visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsMeeItp6p-6"
      },
      "source": [
        "### 5. Define your optimization and evaluation criterion\n",
        "\n",
        "Define an optimizer and criterion (loss function) for your neural network training. To setup your optimizer, you'll have to instantiate your models above, and choose a learning rate. Try a few different optimizers and learning rates to get a sense of what will train within a reasonable timeframe (if your deep network isn't too deep, reaching convergence shouldn't take more than 5-10 minutes with the right choice of learning rate and optimizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQBWlyJcj7w"
      },
      "source": [
        "> **Q5.1) What loss function is suited to this problem?**\n",
        "\n",
        "> MSE works well for image reconstruction and is therefore likely a good loss function for this problem. I also want to try binary cross entropy (BCE) but would need to normalize the pixels to between 0 and 1\n",
        "\n",
        "...\n",
        "\n",
        "> **Q5.2) Try a few optimizers, what seemed to work best?**\n",
        "\n",
        "> I started with the adam optimizer (momentum optimizer) I may try a SGD with and without momentum\n",
        "\n",
        "...\n",
        "\n",
        "> **Q5.3) What's the effect of choosing different batch sizes?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ftxeVJJP6p-7"
      },
      "outputs": [],
      "source": [
        "# Define model, optimizer, loss function, epochs\n",
        "model = HNet().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "n_epochs = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zudx4FKI6p_C"
      },
      "source": [
        "### 6. Run your training loop\n",
        "\n",
        "It's a great idea to monitor the early epochs of your training (\"babysit your training\") to keep an eye on learning. Does the learning rate seem too high? too low?\n",
        "\n",
        "(**Hint: it's recommended that you just test a single epoch at a time while you write your training function, to debug and make sure everything is working appropriately.**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J87zUaXX6p_D",
        "outputId": "729974a6-af85-4d32-c92e-78a859c74c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cuda for 30 epochs...\n",
            "\n",
            "Epoch [1/30], Loss: 0.748794\n",
            "Epoch [2/30], Loss: 0.684024\n",
            "Epoch [3/30], Loss: 0.670025\n",
            "Epoch [4/30], Loss: 0.660117\n",
            "Epoch [5/30], Loss: 0.654652\n",
            "Checkpoint saved at ./checkpoints/autoencoder_epoch_5.pth\n",
            "Epoch [6/30], Loss: 0.652244\n",
            "Epoch [7/30], Loss: 0.649854\n",
            "Epoch [8/30], Loss: 0.647580\n",
            "Epoch [9/30], Loss: 0.645479\n",
            "Epoch [10/30], Loss: 0.644357\n",
            "Epoch [11/30], Loss: 0.643258\n",
            "Epoch [12/30], Loss: 0.641678\n",
            "Epoch [13/30], Loss: 0.640750\n",
            "Epoch [14/30], Loss: 0.640130\n",
            "Epoch [15/30], Loss: 0.639139\n",
            "Epoch [16/30], Loss: 0.638287\n",
            "Epoch [17/30], Loss: 0.638337\n",
            "Epoch [18/30], Loss: 0.636925\n",
            "Epoch [19/30], Loss: 0.635999\n",
            "Epoch [20/30], Loss: 0.635921\n",
            "Epoch [21/30], Loss: 0.634548\n",
            "Epoch [22/30], Loss: 0.635109\n",
            "Epoch [23/30], Loss: 0.634609\n",
            "Epoch [24/30], Loss: 0.633233\n",
            "Epoch [25/30], Loss: 0.632061\n",
            "Epoch [26/30], Loss: 0.632554\n",
            "Epoch [27/30], Loss: 0.631238\n",
            "Epoch [28/30], Loss: 0.630994\n",
            "Epoch [29/30], Loss: 0.630666\n",
            "Epoch [30/30], Loss: 0.630083\n",
            "Checkpoint saved at ./checkpoints/autoencoder_epoch_30.pth\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "loss_history = train(train_loader, model, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoOJQt6vFADr"
      },
      "source": [
        "In your training loop, we requested that you store your training loss for each epoch. Using Matplotlib, please plot your training loss as a function of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "xycLVQK-jwiB",
        "outputId": "5bdce3a9-b17d-4ec1-9cb3-35abe373306b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot Training Loss\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitAd61L6p_M"
      },
      "source": [
        "> **Q6.1)  How do you know when your network is done training?**\n",
        "\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxomi9w5kxIx"
      },
      "source": [
        "Another way to check if your models (`HNet` and `MyNet`) are well trained is to plot a few image reconstructions to see how well your models do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "4pjS79M0oDj_"
      },
      "outputs": [],
      "source": [
        "# extract 6 figures from training DataLoader\n",
        "mini_batch, _ = next(iter(train_loader))\n",
        "n_examples = min(6, mini_batch.shape[0])\n",
        "examples = mini_batch[:n_examples]\n",
        "\n",
        "# compute reconstructions\n",
        "with torch.no_grad():\n",
        "    reconstr_examples = model.forward(\n",
        "        examples.view(n_examples, -1).to(device))\n",
        "\n",
        "# save image with original v. reconstructed images\n",
        "comparison = torch.cat([\n",
        "    examples,\n",
        "    reconstr_examples.view(-1, 1, 28, 28).cpu()])\n",
        "save_image(comparison.cpu(), 'training_reconstruction.png', nrow=n_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "0XG_F4XTqwk9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a75f2079-df40-4d66-8085-bd391ac157ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALYAAAA+CAIAAAAJcXbmAAALsElEQVR4nO2ca0gU3R/HZ1bdXNPH3c1du1h2w7A2FYwkukqQpBRW0MUyjMLA6IUSCL7oTkZREHQhgrAL0YsI6pXdDIKi6GIXKrNSczd1U9M11113bs+LQ6fTzO7snDNHn/8f5vNKZ875nu/5zW9nzjlzYRgDAwMDAwMDAwMDAwMDg5FEkiRJkv6Tpk1aCpWUlJT9xul0lpWVjbCrkcViseip/vz5c3DAMjIyaFlS5/r16yMnDvpCfkwlVWg6HXkOHDiAmr906RKugtPpRBUePXoEti9evLi0tLSxsXFAAZUo6Qw4qFtdXa3c5fV6yZXb29vV80N/ojQ2NqpUX7duncvlIhaXQcW8snpra+vIxUfZKEH19+/fg7p9fX2RxOvr67F1h4eHZf0EP4iwnD59msA69Hfr1i3lruLiYirxRRsCpKWlpaWlnTx5ElcfFbl3717UzLDb7YmJiTqdZ2Zm6kw1i8UCqj99+lS5F+zKzc3FE12+fDnaVVl74I/k5GS0THp6Oq71yspKNOIgzxITE1HZnp4eXFmGYdLT0yMdNrQYbujRYy8LEaCiosJisaSlpRF4jtoocYqoVD927BhhikDRsHmHUltbS9yBSEcRxW6348rC86oKMg+4hhkkCysqKnAdakd/fjCqKUIujlUTFt6xYwdBK+3t7bdu3ZIdwtzc3Ldv3xKkiEznwIEDYbfDrpGliCRJEyZMMJk0TQaJ+fLli/4UUVegkCLFxcXaC+/cuVN7E6WlpWHNJScno7LEKaJSprq6Wvo9x8MNkDLV3r59i+VQIxcvXtSfH3AUIkmSclS0ePHiSLtksMpNDQ0N+fn58N9QKKScl2/dulUuxIaRigTsdqRaoACZ5uTJkz0eT9Tyw8PDZrMZt5X6+vqCggLldiyRqKBpcfDgwX379ukUYRjGarX6fD7l3oSEhEAgQKKOBbG+egHtgiUlJbhmdP5GL126pIxDQ0PD3r17yQSVxoAgsY5yWgr7q+fY/SFsCCKRmZmJJX716lUVf3CFSrsgbod37NihN0Dh1Kho0pKSXa0iQWfxCVUE0xw4X4KcOnWKQFNlL9ZlHiumdrud1uEM60HPNIdiqkFki8Iy9K7uu1wuddPoXqxB/rFjxyLtAl16//69djXtYUXXALXrY9kgTpGRyA8ZNTU1aCt6L4vz58/X4ph6xxoaGiTMs8j379+jeujr64NlKisraTj9CyhOliJoGGtqaqjbg6A/e71aUCjqRYRuipBJSQrS09PT09Nv3rwp2z4Sd2hRfezFSoUCdXso8DyKO4KUg+WYeooMDw/j1srJyVFmiQzqyxjoTApw8uRJAh1UYaQX5WCK6BUiyA+KKaJl1S4sssETROe4fcKECUqTYSEQR/OM4J4DLqOaIhkZGWh0zp07p7dVhjl9+jStVKNIpIRAWbdunX5xurbVm6MmhAKmISaTacGCBbJdBE/oqLQb9Q7iKKOSGX19fatWraIiTsutOrhnkdhIO1iWLS0tVW632+1FRUXM32vwV65cwfQZBa/XS1dQJ6tXr759+7ZsY39/v81m0y9Od/E+KvPmzfv06dNotkiZ0fw9GfxfIkmScmxoYGBgYGBgYGBgYGBgYGBgYGAw6kRZ+jWZTA6Ho7CwcObMmQUFBampqR0dHUlJSR0dHRaL5cyZM9euXRsdo8Qo12qJF7xZlk1OTl69enXYG1I+n89qtZIp/y+jFqzExMSWlhaHw8EwjCRJaGQFQRAE4cePH2azuby8POx7uRgmWJZl2ZFYelcRFATBbDaLoqhRymQyOZ3Ob9++gVcrIml6PJ6pU6fi+lQhJyensbER/K3zbo7JZNq+ffuvX79SU1NTUlKam5uXLVtmNpsfPnx4+fJlnufx5NavX69661uSJEkURY7jPn78SOZ4zJgxRUVFW7ZsqaqqkilPnz49KSlJ//2tqF3QLpWYmCj7/IQSnuefPXum07OK+bFjx5JJ7d+/v729ned5mSDHccFg8NOnT9jPMZWVlUUNLkAQhMHBQVzHOTk5Srsooih++PBh1qxZuMqQSG+RyNAu+M8//7jdblEURVEUBIHjuJ6eHvg5GsCbN2+WLVtG7BlFaZXkEw8MwzDM2rVr1Y+gz+fD/jUqPxXB83x3d3dPT8+dO3cePHgwODgIjyXuCWratGmiKCqNBgIBdLsoisQPISv19aeIxWJxu908z/M87/f76+rqLBZLfHx8VlbWhg0bgNrcuXMTEhLIPMvQYxUlJSUFjUYgEFi0aJHL5dq8eTPc+OTJE2xdWYi9Xm9GRkZMTAzDMElJSdnZ2Xl5eeAogpJYDwyfOnVK1vlgMOh2u+vr630+H7q9tbWV4FoTKRvgsIMs9LGxsd++fQsGgxzHvXjxYvz48SzLjh8/Hp4Rt27dCt9J1g8cnAFA8AlE3G43FCkvL4c6K1asgNvPnj2LLS1LkdLS0ry8vLlz52ZlZW3ZsiUYDKJxF0WxsLBQu3goFILVBUGora212+1JSUkpKSnoaxmSJH39+hX3Ghk2OQRBUCmmUTkuLq6lpSUUCgUCgaamppqamj179tTV1UEdm80WGxvxES1cUIfEH/mJi4sTBAGIHDlyBG43mUzoEZw5cya2dHNzM2oRni1kwI27d++Oj4/XKI4qFBcXT548OdJe3IGw0iHP82HffCdIkSlTpng8HkEQIl3FnE4nlluNHdE+55LBsuyjR4+gCGovJiYGDZH2Y/cH5bsn6gwNDeXl5WkUHxoaghV3796NnkI3bdqEymI9E7py5UqZK5UvI6LFNL7253A4BgYGVEY5N27coHUWQWX1aPb29sIUQaOB9oJgtsEwDHP58mWsFJEkqbe3V+NFwePxcBwHJgVVVVUbN25cuXJlfn7+oUOHUEHcX4/Mj/bC2ofbL1++VB8Id3Z2YnkOC7w0SJLk9XqJX6uJjY2FswoYUlQc8PjxYxL1JUuWoK83amT27NlaxCdNmrRr1y5RFC9cuDBnzhyr1Yp+LAWC+7YVWpduYcjatWtv374Np+scxw0ODsqSRucLO11dXWTelIwZM0amFpY1a9YQNrBt2zaoIv4GbuF5vqury+fzoRuVo0IVYmJiwKKq2WxGR08Qt9uNZVhjZLOzs2UNaW+CZVmbzWaz2SwWCzAPNs6YMQPGgeA9Qsi4ceOIvSkxmUzHjx9Xzw+e54mX45jk5OSvX7/CFOE4zu/3X79+PT4+Hpz6TCZTbW0tOj3R2SWGYebNm0csFdVGZ2enMka4V3qYGTIKCgqgJtkElaG3FgJxOBznz58fGBj4+fPnwMCAz+fzeDyBQADqt7W16Wqgra1taGiI47jh4eH+/v4HDx5kZ2fLRr8cx9HqEroSgHVCAkSycf/+/UgruQT5ERcXF3aX1WqFJxKyFEEXMGilSFgWLlwI9bXMwtRi1NXVNXHixGAwCPr84cMHl8v15s2bv+pTXQmAf3Mch1sdZJhSSqU8bhMMw0QaPG7evBk2TTBNrauro/vBVhVevHgB/pAkqbe3V5fWoUOHXr165fV6/X5/IBDwer0PHz4EN35jYmLQiwKtlNcpFfZUoaS7u5tA3Ol05ufnl5SUZGdno+cJlmUPHz4ML7ihUIiWcwIdLSxduhTo+/1+vVpjx449cuRIc3MzuNwAXY7jZOMPWl1CLzTEIlpShEzZ5XKBV9JRXr9+jf4riiL6sUmNNDU1jVp+MAzz+PFj0ERLSwsFOavVWl5e3tLSAi/ncHZDcRQCoJIiTLgsoXU1jPrp96NHj1LxTMVtJD5//gxauXv3Lh3FOXPmOByOd+/edXd3g+VnsALz8+dPsA7T1tZ28eJF/Q2ZzWZaMQKzaP2WlEQ6g4ZCocLCQuIHXNAVLbqGlfj9fvBTp3jT8Q8nTpw4fPiw3W5PSEhgf0NL3GazjVqYdAJGMyDQra2tWVlZxBPdUYZlWTDpFQRhlL9IQIG4uDiQHwQzXgPtNDU18Tz/48eP/9qIgYGBgYGBgYEG/gWicgSxKx5r6wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 300
            }
          },
          "execution_count": 76
        }
      ],
      "source": [
        "Image('training_reconstruction.png', width=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxNZwHPUGxKu"
      },
      "source": [
        "> **Q6.2) What does `torch.no_grad()` do?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC8PXHyR6p_Q"
      },
      "source": [
        "### 7. Visualize the learning process\n",
        "\n",
        "We'll next try to visualize how well the model is learning on the **test set**. To do this, we'll first visualize the \"learning process\" by viewing reconstruction at various stages.\n",
        "\n",
        "* Using your checkpoints saved during training, plot a batch of images from the test set and their corresponding reconstructions based on each of your saved models over time. You should see the quality of the reconstructions improving over time.\n",
        "* To visualize images, you can use the helper functions provided below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDO9E6Vl6p_N"
      },
      "outputs": [],
      "source": [
        "### Helper Functions for Plotting Multiple Images\n",
        "\n",
        "def imshow(inp,\n",
        "           figsize=(10,10),\n",
        "           mean=0.1307, # for MNIST train\n",
        "           std=0.3081, # for MNIST train\n",
        "           title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.cpu().detach()\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array(mean)\n",
        "    std = np.array(std)\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "def reconstructions_from_batch(model, batch):\n",
        "    batch = batch.view(-1, 28 * 28).to(device)\n",
        "    reconstruction = model(batch)\n",
        "    return reconstruction.reshape(batch.shape[0],1,28,28)\n",
        "\n",
        "# Get a batch of training data\n",
        "batch, classes = next(iter(test_loader))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(batch)\n",
        "imshow(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2edUEVG3Rev"
      },
      "outputs": [],
      "source": [
        "# Define function to load a checkpoint and generate reconstructions\n",
        "def load_checkpoint_and_reconstruct(model, test_loader, checkpoint_epochs, save_dir=\"./checkpoints\"):\n",
        "    for epoch in checkpoint_epochs:\n",
        "        checkpoint_path = os.path.join(save_dir, f\"autoencoder_epoch_{epoch}.pth\")\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "            model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "            model.eval()  # Set model to evaluation mode\n",
        "\n",
        "            # Get a batch of test data\n",
        "            batch, _ = next(iter(test_loader))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                reconstructions = model(batch.view(batch.size(0), -1).to(device))\n",
        "\n",
        "            # Prepare for visualization\n",
        "            comparison = torch.cat([batch, reconstructions.view(-1, 1, 28, 28).cpu()])\n",
        "            save_image(comparison, f\"{checkpoint_path}_reconstruction.png\", nrow=batch.size(0))\n",
        "\n",
        "            print(f\"Saved reconstruction comparison for {checkpoint_path}\")\n",
        "\n",
        "# List of checkpoint epochs (same as in training function)\n",
        "checkpoint_epochs = [5, 30, 60]\n",
        "\n",
        "# Run function to load and reconstruct\n",
        "load_checkpoint_and_reconstruct(model, test_loader, checkpoint_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhuSg4D6p_R"
      },
      "source": [
        "### 8. Visualize the latent space\n",
        "\n",
        "As discussed in class, the first half of an autoencoder (the *encoder*) maps the original input into a lower-dimensional latent space.\n",
        "* Just as shown in Hinton and Salakhutdinov, run your test set of 10,000 MNIST digits through the **encoding layer** of one of the trained networks above. Each sample should readily map to a 2-dimension point. To do this, it will be helpful to fill out a new function, **encode** below, that takes in your trained model and the `test_dataloader` to produce 2d latent embeddings and their corresponding labels.\n",
        "* Plot each point in these two dimensions, and color each point in this **latent space** by their known **labels**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNg0PfrK4gzn"
      },
      "outputs": [],
      "source": [
        "def encode(model, device, test_loader):\n",
        "    model.eval()\n",
        "    latent_embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, batch_labels in test_loader:\n",
        "            batch = batch.view(batch.size(0), -1).to(device)\n",
        "            encoded = model.encoder(batch)  # Extract latent space representation\n",
        "            latent_embeddings.append(encoded.cpu().numpy())\n",
        "            labels.append(batch_labels.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    latent_embeddings = np.concatenate(latent_embeddings, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "    return latent_embeddings, labels\n",
        "\n",
        "# Get latent space embeddings\n",
        "latent_embeddings, labels = encode(model, device, test_loader)\n",
        "\n",
        "# Plot latent space using matplotlib\n",
        "plt.figure(figsize=(10, 8))\n",
        "for digit in range(10):\n",
        "    idx = labels == digit  # Find all points for this digit\n",
        "    plt.scatter(latent_embeddings[idx, 0], latent_embeddings[idx, 1], label=str(digit), alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Latent Dimension 1\")\n",
        "plt.ylabel(\"Latent Dimension 2\")\n",
        "plt.title(\"Latent Space Representation (Color-coded by Label)\")\n",
        "plt.legend(title=\"Digits\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGkujKgGPEi"
      },
      "source": [
        "> **Q8.1) Does your autoencoder separate out different classes effectively? What classes seem to be closer and what classes are farther apart in this latent space?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78___-BU6p_S"
      },
      "source": [
        "## Optional (advanced): Train an autoencoder on CelebA Faces\n",
        "\n",
        "Real-world images tend to be far more complex than digits from MNIST. As an optional exercise for your own interest, or for students looking for more experience, we'll investigate a subset of CelebA below.\n",
        "\n",
        "We provide the images in a .zip file (`faces.zip`) in the class's Google Drive folder, which contains a \"train\" and \"test\" set of 80k and 10k images, respectively. Although these are color, RGB images, below we've set up the datasets to convert these to grayscale with precomputed means (0.4401) and stds (0.2407), for convenience and easier compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K6o1Wr88EXj"
      },
      "outputs": [],
      "source": [
        "### Download faces.zip and unzip it into bmi219_downloads/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM7WXmAu6p_T"
      },
      "outputs": [],
      "source": [
        "preprocessing = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4401,), (0.2407,)),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    'bmi219_downloads/Faces/train',\n",
        "    transform=preprocessing)\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    'bmi219_downloads/Faces/test',\n",
        "    transform=preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQn4cgBW6p_X"
      },
      "source": [
        "As above, you'll want to:\n",
        "\n",
        "1. set up your dataloaders and visualize some of the images\n",
        "2. set up your autoencoder network architecture\n",
        "3. define your training criterion and optimizer\n",
        "4. train your network\n",
        "    \n",
        "In this case, you should be able to reuse much of your code from above. Consider a few questions:\n",
        "\n",
        "1. How well do complex images like faces work with a latent dimension of 2?\n",
        "2. Do reconstructions look better with a larger bottleneck?\n",
        "3. What kind of features are poorly reconstructed? What happens to sunglasses, hats, and hands?\n",
        "4. Try sampling the 2-d latent space close to existing examples (by adding some noise...) or randomly. What do the generated images look like?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}